{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae789ba5",
   "metadata": {},
   "source": [
    "# Conclusion ğŸ¯\n",
    "\n",
    "In practice, the **Decision Tree Regressor** outperformed **Linear Regression**, **Lasso Regression**, and **Ridge Regression**, achieving the best results on the insurance dataset. This superior performance can be attributed to several factors:\n",
    "\n",
    "- **Non-linear Relationships** ğŸ”€  \n",
    "  Decision trees can capture complex, non-linear relationships between features and the target variable, while linear models assume a straight-line relationship.\n",
    "\n",
    "- **Feature Interactions** ğŸ¤  \n",
    "  Decision trees automatically model interactions between features, which linear models do not unless you explicitly add interaction terms.\n",
    "\n",
    "- **Outliers and Skewed Data** ğŸ“Š  \n",
    "  Decision trees are less sensitive to outliers and skewed distributions, which can negatively impact linear models.\n",
    "\n",
    "**However** âš ï¸  \n",
    "Decision trees can easily overfit, especially on small datasets or if not properly tuned.\n",
    "\n",
    "**Tuning hyperparameters ğŸ› ï¸**  \n",
    "By tuning the hyperparameters `max_depth` and `min_samples_leaf`, the model achieved better generalization to unseen data, leading to more robust and consistent performance on the test set. âœ…"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
